# ollama
Your own LLM portable

By default, the used model is mistral and Ollama acts as Gandalf.

You can replace the pre-prompt in the docker-compose.yaml file, replacing the NEXT_PUBLIC_DEFAULT_SYSTEM_PROMPT.

To run this project you only need docker installed in your laptop and execute:

```make launch``` 
