# ollama
Your own LLM portable

By default, the used model is mistral and Ollama acts as Gandalf.

You can replace the pre-prompt in the docker-compose.yaml file, replacing the NEXT_PUBLIC_DEFAULT_SYSTEM_PROMPT.

Use the model "Llama2" and modify your docker settings, the resources, increasing the RAM as much as possible for better speeds.

To run this project you only need docker installed in your laptop and execute:

```make launch``` 
